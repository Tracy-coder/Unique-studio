# 决策树算法
## [功能描述]：利用决策树算法实现对diabetes数据集的分类
## [决策树算法实现]：
### <u>ID3：</u>
##### 1.Entropy熵：
熵用于表示**随机变量不确定性的度量**
X是一个取i个值的离散随机变量
![](C:\Users\86189\Desktop\20180825124137815 (1).png)
熵越大，不确定性越大
##### 2.conditional entropy条件熵：
随机变量X给定的条件下，随机变量Y的条件熵H(Y|X)![]()

##### 3.Information Gain信息增益：
信息增益表示的是**得知特征X的信息而使得类Y的信息的不确定性减少的程度**
<img src="C:\Users\86189\Desktop\993405-20161013113413078-639880046.png" style="zoom: 67%;" />
在ID3算法中，遍历所有属性分别计算信息增益，信息增益最大的那个属性将作为划分属性。


### <u>C4.5：</u>
##### information gain ratio信息增益率：
使用信息增益来选择划分属性时，它偏向于选择那些取值多的属性进行划分，显然这样的划分不具有良好的泛化能力。于是有了使用增益率来选择划分属性。增益率定义如下：![](C:\Users\86189\Desktop\QQ图片20201025161249.png)

在C4.5算法中，遍历所有属性分别计算信息增益率，信息增益率最大的那个属性将作为划分属性。



### <u>CART：</u>
##### Gini基尼值：
基尼值反映了从数据集中随机抽取两个样本，其类别标记不一致的概率，因此基尼值越小，则数据集纯度越高。![](C:\Users\86189\Desktop\QQ图片20201025161634.png)
##### Gini指数：![](C:\Users\86189\Desktop\QQ图片20201025161639.png)

在CART算法中，遍历所有属性分别计算基尼指数，基尼指数最小的那个属性将作为划分属性。

##### 连续值处理：
基于某一划分点t可以将数据集D划分为 两部分 ，他们分别包含那些属性上取值不大于t的样本和大于t的样本。因此对任一连续属性我们有n-1个候选划分节点：![](C:\Users\86189\Desktop\QQ图片20201025162014.png)

再利用基尼指数，在连续取值中选择一个最佳分裂节点


## [使用步骤]：
	1.导入训练数据
	2.构建决策树
	3.决策树的可视化
	4.进行分类
	(5.) 进行剪枝